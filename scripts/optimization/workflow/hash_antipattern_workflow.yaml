# This is a sample workflow to test or replace with your source code.
#
# This workflow passes the region where the workflow is deployed
# to the Wikipedia API and returns a list of related Wikipedia articles.
# A region is retrieved from the GOOGLE_CLOUD_LOCATION system variable
# unless you input your own search term; for example, {"searchTerm": "asia"}.
main:
    params: [input]
    steps:
    - assign_vars:
        assign:
        - queryHash: "DECLARE num_days_to_scan INT64 DEFAULT 30;"
        - queryHash: ${queryHash+" "+ "CREATE TEMP FUNCTION num_stages_with_perf_insights(query_info ANY TYPE) AS ("}
        - queryHash: ${queryHash+" "+ "COALESCE(("}
        - queryHash: ${queryHash+" "+ "SELECT SUM(IF(i.slot_contention, 1, 0) + IF(i.insufficient_shuffle_quota, 1, 0))"}
        - queryHash: ${queryHash+" "+ "FROM UNNEST(query_info.performance_insights.stage_performance_standalone_insights) i), 0)"}
        - queryHash: ${queryHash+" "+ "+ COALESCE(ARRAY_LENGTH(query_info.performance_insights.stage_performance_change_insights), 0)"}
        - queryHash: ${queryHash+" "+ ");"}
        - queryHash: ${queryHash+" "+ "CREATE SCHEMA IF NOT EXISTS optimization_workshop;"}
        - queryHash: ${queryHash+" "+ "CREATE OR REPLACE TABLE optimization_workshop.queries_grouped_by_hash AS"}
        - queryHash: ${queryHash+" "+ "SELECT"}
        - queryHash: ${queryHash+" "+ "statement_type,"}
        - queryHash: ${queryHash+" "+ "query_info.query_hashes.normalized_literals                              AS query_hash,"}
        - queryHash: ${queryHash+" "+ "COUNT(DISTINCT DATE(start_time))                                         AS days_active,"}
        - queryHash: ${queryHash+" "+ "ARRAY_AGG(DISTINCT project_id IGNORE NULLS)                              AS project_ids,"}
        - queryHash: ${queryHash+" "+ "ARRAY_AGG(DISTINCT reservation_id IGNORE NULLS)                          AS reservation_ids,"}
        - queryHash: ${queryHash+" "+ "SUM(num_stages_with_perf_insights(query_info))                           AS num_stages_with_perf_insights,"}
        - queryHash: ${queryHash+" "+ "COUNT(DISTINCT (project_id || ':us.' || job_id))                         AS job_count,"}
        - queryHash: ${queryHash+" "+ "ARRAY_AGG("}
        - queryHash: ${queryHash+" "+ "STRUCT("}
        - queryHash: ${queryHash+" "+ "bqutil.fn.job_url(project_id || ':us.' || parent_job_id) AS parent_job_url,"}
        - queryHash: ${queryHash+" "+ "bqutil.fn.job_url(project_id || ':us.' || job_id) AS job_url,"}
        - queryHash: ${queryHash+" "+ "query as query_text"}
        - queryHash: ${queryHash+" "+ ")"}
        - queryHash: ${queryHash+" "+ "ORDER BY total_slot_ms"}
        - queryHash: ${queryHash+" "+ "DESC LIMIT 10)                                                         AS top_10_jobs,"}
        - queryHash: ${queryHash+" "+ "ARRAY_AGG(DISTINCT user_email)                                           AS user_emails,"}
        - queryHash: ${queryHash+" "+ "SUM(total_bytes_processed) / POW(1024, 3)                                AS total_gigabytes_processed,"}
        - queryHash: ${queryHash+" "+ "AVG(total_bytes_processed) / POW(1024, 3)                                AS avg_gigabytes_processed,"}
        - queryHash: ${queryHash+" "+ "SUM(total_slot_ms) / (1000 * 60 * 60)                                    AS total_slot_hours,"}
        - queryHash: ${queryHash+" "+ "AVG(total_slot_ms) / (1000 * 60 * 60)                                    AS avg_total_slot_hours_per_active_day,"}
        - queryHash: ${queryHash+" "+ "AVG(TIMESTAMP_DIFF(end_time, start_time, SECOND) )                       AS avg_job_duration_seconds,"}
        - queryHash: ${queryHash+" "+ "ARRAY_AGG(DISTINCT FORMAT('%T',labels))                                  AS labels,"}
        - queryHash: ${queryHash+" "+ "SUM(total_slot_ms / TIMESTAMP_DIFF(end_time, start_time, MILLISECOND))   AS total_slots,"}
        - queryHash: ${queryHash+" "+ "AVG(total_slot_ms / TIMESTAMP_DIFF(end_time, start_time, MILLISECOND))   AS avg_total_slots,"}
        - queryHash: ${queryHash+" "+ "ANY_VALUE(ARRAY("}
        - queryHash: ${queryHash+" "+ "SELECT"}
        - queryHash: ${queryHash+" "+ "ref_table.project_id || '.' ||"}
        - queryHash: ${queryHash+" "+ "IF(STARTS_WITH(ref_table.dataset_id, '_'), 'TEMP', ref_table.dataset_id)"}
        - queryHash: ${queryHash+" "+ "|| '.' || ref_table.table_id"}
        - queryHash: ${queryHash+" "+ "FROM UNNEST(referenced_tables) ref_table"}
        - queryHash: ${queryHash+" "+ "))                                                                       AS referenced_tables,"}
        - queryHash: ${queryHash+" "+ "FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT"}
        - queryHash: ${queryHash+" "+ "WHERE"}
        - queryHash: ${queryHash+" "+ "DATE(creation_time) >= CURRENT_DATE - num_days_to_scan"}
        - queryHash: ${queryHash+" "+ "AND state = 'DONE'"}
        - queryHash: ${queryHash+" "+ "AND error_result IS NULL"}
        - queryHash: ${queryHash+" "+ "AND job_type = 'QUERY'"}
        - queryHash: ${queryHash+" "+ "AND statement_type != 'SCRIPT'"}
        - queryHash: ${queryHash+" "+ "GROUP BY statement_type, query_hash;"}
        - queryRaw: "CREATE OR REPLACE TABLE optimization_workshop.hash_raw AS (SELECT query_hash as id,top_10_jobs[OFFSET(0)].query_text as query FROM optimization_workshop.queries_grouped_by_hash)"
    - runHashQuery:
        call: googleapis.bigquery.v2.jobs.query
        args:
            projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            body:
                useLegacySql: false
                # Query name and gender of most popular names
                query: ${queryHash}
    - runRawQuery:
        call: googleapis.bigquery.v2.jobs.query
        args:
            projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            body:
                useLegacySql: false
                # Query name and gender of most popular names
                query: ${queryRaw}
    - runDeleteQuery:
        call: googleapis.bigquery.v2.jobs.query
        args:
            projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            body:
                useLegacySql: false
                # Query name and gender of most popular names
                query: "delete from `wpe-tam-sandbox-anthos.optimization_workshop.hash_raw` where id is NULL"
    - run_job:
        call: googleapis.run.v1.namespaces.jobs.run
        args:
            name: "namespaces/wpe-tam-sandbox-anthos/jobs/bigquery-antipattern-recognition"
            location: "us-central1"
    - joinAntiPattern:
        call: googleapis.bigquery.v2.jobs.query
        args:
            projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            body:
                useLegacySql: false
                # Query name and gender of most popular names
                query: "CREATE OR REPLACE TABLE optimization_workshop.test AS (select r.*, o.recommendation from optimization_workshop.queries_grouped_by_hash as r full outer join optimization_workshop.antipattern_output_table as o ON r.query_hash = o.job_id)"
           